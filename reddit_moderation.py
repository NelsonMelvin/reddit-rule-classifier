# -*- coding: utf-8 -*-
import streamlit as st
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ydBjgVhsmWYXtJGFIbhoznb02R6UbEy-
"""

import pandas as pd
import random

# Sample subreddit topics
subreddits = ['politics', 'gaming', 'relationships', 'news', 'funny']

# Clean comments
clean_comments = {
    'politics': [
        "I appreciate the different point of view.",
        "Interesting how policies evolve over time.",
        "Thanks for sharing this article.",
        "This debate was surprisingly civil.",
        "Great breakdown of the election results."
    ],
    'gaming': [
        "This game has amazing graphics!",
        "Loved the new update.",
        "Nice strategy, I'll try it out.",
        "Great team coordination in this match.",
        "I‚Äôve been waiting for this release!"
    ],
    'relationships': [
        "You gave great advice here.",
        "Glad things worked out for you.",
        "Communication is definitely key.",
        "Thanks for being open about your journey.",
        "That was a really thoughtful response."
    ],
    'news': [
        "Very informative report.",
        "This gives a new perspective on the issue.",
        "Interesting piece, thanks for posting.",
        "The facts were laid out well.",
        "It‚Äôs good to stay informed."
    ],
    'funny': [
        "This made me laugh so hard!",
        "10/10 comedy!",
        "Perfect timing üòÇ",
        "That‚Äôs actually hilarious!",
        "I can't stop watching this lol"
    ]
}

# Rule-breaking comments
toxic_comments = {
    'politics': [
        "You‚Äôre just a brain-dead sheep.",
        "Only an idiot would believe that.",
        "Go cry somewhere else, clown.",
        "You're what's wrong with this country.",
        "This post is absolute garbage."
    ],
    'gaming': [
        "Trash gameplay like always.",
        "You suck. Uninstall the game.",
        "Only noobs use that weapon.",
        "Typical loser move.",
        "You're the worst teammate ever."
    ],
    'relationships': [
        "That‚Äôs the dumbest advice ever.",
        "No wonder you got dumped.",
        "You're pathetic.",
        "This is why no one likes you.",
        "Go fix your own mess first."
    ],
    'news': [
        "Fake news, shut up.",
        "You sound like a government puppet.",
        "Go educate yourself, moron.",
        "Stop spreading BS propaganda.",
        "This post is complete trash."
    ],
    'funny': [
        "That wasn‚Äôt funny, idiot.",
        "You must have zero brain cells.",
        "Lame. Try harder next time.",
        "Please delete your account.",
        "This joke is so bad it hurts."
    ]
}

# Generate dataset
comments = []
labels = []
subs = []

for sub in subreddits:
    for _ in range(40):  # 20 clean + 20 toxic per subreddit
        comments.append(random.choice(clean_comments[sub]))
        labels.append(0)
        subs.append(sub)
        comments.append(random.choice(toxic_comments[sub]))
        labels.append(1)
        subs.append(sub)

# Shuffle dataset
data = list(zip(comments, labels, subs))
random.shuffle(data)
comments, labels, subs = zip(*data)

df = pd.DataFrame({
    "comment": comments,
    "label": labels,
    "subreddit": subs
})

df.head()

import string
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')

def clean_text(text):
    stop_words = set(stopwords.words('english'))
    text = text.lower()
    text = "".join([char for char in text if char not in string.punctuation])
    words = text.split()
    words = [word for word in words if word not in stop_words]
    return " ".join(words)

df["cleaned"] = df["comment"].apply(clean_text)

# One-hot encode the subreddit column
df_encoded = pd.get_dummies(df, columns=["subreddit"])

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from scipy.sparse import hstack

# TF-IDF vectorizer
vectorizer = TfidfVectorizer(max_features=1000)
X_text = vectorizer.fit_transform(df_encoded["cleaned"])

# Optional: Add subreddit features
subreddit_features = df_encoded.drop(columns=["comment", "label", "cleaned"])
X_full = hstack([X_text, subreddit_features.values])

# Labels
y = df_encoded["label"]

# Split
X_train, X_test, y_train, y_test = train_test_split(X_full, y, test_size=0.2, random_state=42)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

model = LogisticRegression(max_iter=200)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))

import pandas as pd

# Example: Generate sample submission
# Replace this with your actual test data and predictions if available
submission = pd.DataFrame({
    'comment_id': ['c1', 'c2', 'c3', 'c4'],
    'rule_violation': [0, 1, 0, 1]
})

# Save as CSV
submission.to_csv('submission.csv', index=False)

# Confirm it's saved
submission.head()


"""# üõ°Ô∏è Reddit Rule Violation Classifier
This project builds a machine learning model to detect whether Reddit comments break subreddit rules. Inspired by real-world moderation challenges, the model is trained on a synthetic but realistic dataset.

## üìò Project Overview
This project uses a synthetic dataset that mimics real Reddit comments to train a binary classifier that detects whether a comment violates subreddit rules. It demonstrates skills in:
- Text preprocessing (NLP)
- Feature engineering
- Model training with scikit-learn
- Evaluation and performance metrics

This project was originally inspired by a Kaggle competition but is now repurposed for portfolio demonstration.

## üß™ Generate a Realistic Dataset

Since we are not using real Reddit data, we generate a synthetic dataset that simulates Reddit comments and labels:
- 0 ‚Üí Safe comment
- 1 ‚Üí Rule-breaking comment

We also include subreddit names to simulate community context.

## üßº Text Preprocessing

We clean the comment text using NLP techniques:
- Lowercasing
- Removing punctuation
- Tokenizing
- Removing stopwords

This helps the model better understand the patterns in the text.

## üìä TF-IDF Vectorization and Splitting

We transform the cleaned text into numerical features using TF-IDF (Term Frequency‚ÄìInverse Document Frequency). Then, we split the data into training and testing sets to evaluate performance.

## ü§ñ Train and Evaluate the Model

We use a simple but effective model (e.g., Logistic Regression or Naive Bayes) to classify the comments. Then, we evaluate the model using accuracy, precision, recall, and F1 score.
"""

st.markdown("## üß† Try It Yourself!")
st.markdown("Paste a Reddit comment below and find out if it violates subreddit rules.")

# Text input box
user_input = st.text_area("‚úèÔ∏è Enter a Reddit comment:")

# Check button
if st.button("üîç Check for Violation"):
    if user_input.strip() == "":
        st.warning("Please enter a comment to check.")
    else:
        # Preprocess and predict
        user_vector = vectorizer.transform([user_input])
        prediction = model.predict(user_vector)[0]

        # Show result
        if prediction == 1:
            st.error("üö´ This comment *violates* subreddit rules.")
        else:
            st.success("‚úÖ This comment is *safe*.")
